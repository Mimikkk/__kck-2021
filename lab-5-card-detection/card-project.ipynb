{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports used in the project."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pathlib\n",
    "import base64\n",
    "from typing import *\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "\n",
    "import io\n",
    "import keras\n",
    "from PIL import Image\n",
    "import xmltodict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Data Sets.\n",
    "- first set : [Playing Cards Labelized Dataset](https://www.kaggle.com/hugopaigneau/playing-cards-dataset).\n",
    "- second set : [Playing Cards](https://www.kaggle.com/vdntdesai11/playing-cards).\n",
    "\n",
    "## Loading.\n",
    "\n",
    "### Playing Cards\n",
    "\n",
    "#### Classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/12000 records written.\n",
      "200/12000 records written.\n",
      "300/12000 records written.\n",
      "400/12000 records written.\n",
      "500/12000 records written.\n",
      "600/12000 records written.\n",
      "700/12000 records written.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\HOUOUI~1\\AppData\\Local\\Temp/ipykernel_14992/1837562295.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[0mxml_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mimage_count\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    144\u001B[0m \u001B[0mxml_records\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mXMlRecordProvider\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_record\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mxmls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 145\u001B[1;33m \u001B[0mFeatureWriter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcombine_write\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxml_records\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mxml_records\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mPaths\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'record'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mxml_count\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    146\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\HOUOUI~1\\AppData\\Local\\Temp/ipykernel_14992/1837562295.py\u001B[0m in \u001B[0;36mcombine_write\u001B[1;34m(record_iterators, target, total)\u001B[0m\n\u001B[0;32m    130\u001B[0m       \u001B[1;32mfor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecord\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0miterator\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrecord_iterators\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mrecord\u001B[0m \u001B[1;32min\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    131\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;36m100\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{index}{f'/{total}' if total else ''} records written.\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 132\u001B[1;33m         \u001B[0mwriter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSerializeToString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    133\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mFeatureReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobject\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    134\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mclassmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\kck-laboratory\\venv\\lib\\site-packages\\tensorflow\\python\\lib\\io\\tf_record.py\u001B[0m in \u001B[0;36mwrite\u001B[1;34m(self, record)\u001B[0m\n\u001B[0;32m    311\u001B[0m       \u001B[0mrecord\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    312\u001B[0m     \"\"\"\n\u001B[1;32m--> 313\u001B[1;33m     \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mTFRecordWriter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    314\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    315\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mflush\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "Paths: Dict[str, str] = {\n",
    "  'resources': 'resources/',\n",
    "  'cards': 'resources/cards/',\n",
    "  'yolo': 'resources/cards/yolo-labeled/',\n",
    "  'xml': 'resources/cards/xml-labeled/',\n",
    "  'record': 'resources/cards/cards'\n",
    "}\n",
    "Classes = {\n",
    "  'As': 1, 'Ac': 13, 'Ad': 26, 'Ah': 39,\n",
    "  'Ks': 2, 'Kc': 14, 'Kd': 27, 'Kh': 40,\n",
    "  'Qs': 3, 'Qc': 15, 'Qd': 28, 'Qh': 41,\n",
    "  'Js': 4, 'Jc': 16, 'Jd': 29, 'Jh': 42,\n",
    "  '10s': 5, '10c': 17, '10d': 30, '10h': 43,\n",
    "  '9s': 6, '9c': 18, '9d': 31, '9h': 44,\n",
    "  '8s': 7, '8c': 19, '8d': 32, '8h': 45,\n",
    "  '7s': 8, '7c': 20, '7d': 33, '7h': 46,\n",
    "  '6s': 9, '6c': 21, '6d': 34, '6h': 47,\n",
    "  '5s': 10, '5c': 22, '5d': 35, '5h': 48,\n",
    "  '4s': 11, '4c': 23, '4d': 36, '4h': 49,\n",
    "  '3s': 12, '3c': 24, '3d': 37, '3h': 50,\n",
    "  '2s': 25, '2c': 38, '2d': 51, '2h': 52,\n",
    "}\n",
    "\n",
    "Xml = Dict[str, 'Xml']\n",
    "class FileReader(object):\n",
    "  @classmethod\n",
    "  def read(cls, filepath: str) -> bytes:\n",
    "    with tf.io.gfile.GFile(filepath, 'rb') as file: return file.read()\n",
    "class YoloReader(FileReader):\n",
    "  pass\n",
    "class XmlReader(FileReader):\n",
    "  @classmethod\n",
    "  def parse(cls, xmlpath: str) -> Xml:\n",
    "    return xmltodict.parse(super().read(xmlpath))\n",
    "class ImageReader(FileReader):\n",
    "  @classmethod\n",
    "  def encoded_file(cls, imagepath: str) -> bytes:\n",
    "    raw = super().read(imagepath)\n",
    "    return base64.b85encode(raw).decode('utf-8')\n",
    "\n",
    "  @classmethod\n",
    "  def encoded(cls, image: Image.Image) -> bytes:\n",
    "    with io.BytesIO() as buffer:\n",
    "      image.save(buffer, image.format)\n",
    "      return base64.b85encode(buffer.getvalue())\n",
    "\n",
    "  @classmethod\n",
    "  def decoded(cls, imageraw: str) -> Image.Image:\n",
    "    return Image.open(io.BytesIO(base64.b85decode(imageraw)))\n",
    "\n",
    "  @classmethod\n",
    "  def read(cls, imagepath: str) -> Image.Image:\n",
    "    return Image.open(io.BytesIO(super().read(imagepath)))\n",
    "\n",
    "  @classmethod\n",
    "  def read_resized(cls, imagepath: str, size: Tuple[int, int] = (400, 400)) -> Image.Image:\n",
    "    image = cls.read(imagepath)\n",
    "    new = image\n",
    "    new = new.resize(size, Image.ANTIALIAS)\n",
    "    new.format = image.format\n",
    "    return new\n",
    "\n",
    "class FeatureProvider(object):\n",
    "  @classmethod\n",
    "  def i64(cls, value) -> tf.train.Feature:\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "  @classmethod\n",
    "  def i64_list(cls, value) -> tf.train.Feature:\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "  @classmethod\n",
    "  def u8(cls, value) -> tf.train.Feature:\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "  @classmethod\n",
    "  def u8_list(cls, value) -> tf.train.Feature:\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "  @classmethod\n",
    "  def f32(cls, value) -> tf.train.Feature:\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "  @classmethod\n",
    "  def f32_list(cls, value) -> tf.train.Feature:\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "class RecordProvider(FeatureProvider):\n",
    "  @classmethod\n",
    "  def record(cls, features: Dict[str, tf.train.Feature]) -> tf.train.Example:\n",
    "    return tf.train.Example(features=tf.train.Features(feature=features))\n",
    "class XMlRecordProvider(RecordProvider):\n",
    "  @classmethod\n",
    "  def to_record(cls, image: Image.Image, xml: Xml) -> tf.train.Example:\n",
    "    return cls.record(cls._parse(image, xml))\n",
    "\n",
    "  @classmethod\n",
    "  def _parse(cls, image: Image.Image, xml: Xml):\n",
    "    filename = xml['annotation']['filename']\n",
    "    (width, height) = map(int, (xml['annotation']['size']['width'], xml['annotation']['size']['height']))\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for card in xml['annotation']['object']:\n",
    "      xmins.append(int(card['bndbox']['xmin']) / width)\n",
    "      xmaxs.append(int(card['bndbox']['xmax']) / width)\n",
    "      ymins.append(int(card['bndbox']['ymin']) / height)\n",
    "      ymaxs.append(int(card['bndbox']['ymax']) / height)\n",
    "      texts.append(card['name'].encode('utf8'))\n",
    "      labels.append(Classes[card['name']])\n",
    "\n",
    "    return {\n",
    "      'name': cls.u8(filename.encode('utf8')),\n",
    "      'encoded': cls.u8(ImageReader.encoded(image)),\n",
    "      'format': cls.u8(image.format.encode('utf8')),\n",
    "      'height': cls.i64(image.height),\n",
    "      'width': cls.i64(image.width),\n",
    "      'cards/bbox/xmin': cls.f32_list(xmins),\n",
    "      'cards/bbox/xmax': cls.f32_list(xmaxs),\n",
    "      'cards/bbox/ymin': cls.f32_list(ymins),\n",
    "      'cards/bbox/ymax': cls.f32_list(ymaxs),\n",
    "      'cards/class/text': cls.u8_list(texts),\n",
    "      'cards/class/label': cls.i64_list(labels),\n",
    "    }\n",
    "\n",
    "class FeatureWriter(object):\n",
    "  @staticmethod\n",
    "  def write(records: Iterable[tf.train.Example], target: str):\n",
    "    with tf.io.TFRecordWriter(f\"{target}.record\") as writer:\n",
    "      for record in records: writer.write(record.SerializeToString())\n",
    "\n",
    "  @staticmethod\n",
    "  def combine_write(record_iterators: Iterable[Iterable[tf.train.Example]], target: str, total: int = None):\n",
    "    with tf.io.TFRecordWriter(f\"{target}.record\") as writer:\n",
    "      for (index, record) in enumerate((record for iterator in record_iterators for record in iterator), start=1):\n",
    "        if (index % 100) == 0: print(f\"{index}{f'/{total}' if total else ''} records written.\")\n",
    "        writer.write(record.SerializeToString())\n",
    "class FeatureReader(object):\n",
    "  @classmethod\n",
    "  def read(cls, source: str) -> tf.data.TFRecordDataset:\n",
    "    return tf.data.TFRecordDataset(f\"{source}.record\")\n",
    "\n",
    "xml_directory = pathlib.Path(Paths['xml'])\n",
    "image_count = len(list(xml_directory.glob('*.jpg')))\n",
    "images = map(ImageReader.read, xml_directory.glob('*.jpg'))\n",
    "xmls = map(XmlReader.parse, xml_directory.glob('*.xml'))\n",
    "\n",
    "xml_count = 2 * image_count\n",
    "xml_records = map(lambda data: XMlRecordProvider.to_record(*data), zip(images, xmls))\n",
    "FeatureWriter.combine_write((xml_records, xml_records), Paths['record'], total=xml_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# print(f\"|> Training set row example <|\")\n",
    "# print(dfs['train'].head())\n",
    "#\n",
    "# print(f\"|> Test set row example <|\")\n",
    "# print(dfs['test'].head())\n",
    "# print(len(dfs['test']))\n",
    "#\n",
    "# print(f\"|> Dataset classes <|\")\n",
    "# print(dfs['train']['class'].unique())\n",
    "# print(f\"len(dfs['train'])\")\n",
    "#\n",
    "# print(\"|> Example paths <|\")\n",
    "# for cardpath in cardpaths.take(5): print(cardpath.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmenting.\n",
    "\n",
    "### Read saved data from record"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = tf.data.TFRecordDataset(Paths['record'])\n",
    "\n",
    "for data in test_data.take(1):\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(data.numpy())\n",
    "  bytes_ = example.features.feature['image/encoded'].bytes_list.value[0]\n",
    "  Image.open(io.BytesIO(bytes_)).save('temp.png')\n",
    "  print(example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading augmented."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "model: keras.Sequential = keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1. / 255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(len(Classes))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compiling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  dfs['train']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validating."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wniosking."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}